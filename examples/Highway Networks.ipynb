{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highway networks\n",
    "**A quick example of how to implement [highway networks](http://arxiv.org/abs/1505.00387) in Lasagne.** This uses the code from the MNIST example bundled with Lasagne, so you will need to have the `mnist.py` file available for importing.\n",
    "\n",
    "## What's a highway network?\n",
    "The paper linked above introduces a new type of neural network layer, which works roughly as follows.\n",
    "\n",
    "Let `x` be the input to a layer. Then a typical neural network layer computes some nonlinear transform of this input `y = H(x)`.\n",
    "\n",
    "A highway layer also computes an additional nonlinear transform `T(x)`, which in practice is constrained to the interval `[0, 1]`. The output of the layer is then `y = T(x) * H(x) + (1 - T(x)) * x`, where the multiplication is elementwise.\n",
    "\n",
    "In other words, **depending on the gate values, the layer behaves as a traditional layer would (`T(x) = 1`), or passes its input through unchanged (`T(x) = 0`)**. This idea is inspired by the gates in LSTM units. According to the authors, **it enables gradient descent-based training of much deeper networks** with as many as 900 layers.\n",
    "\n",
    "Note that a highway layer needs to have as many outputs as inputs: the shapes of `x`, `H(x)` and `T(x)` all have to match. To change the dimensionality in a highway network, the authors suggest inserting a traditional neural network layer.\n",
    "\n",
    "## Purpose\n",
    "I read the paper before and wanted to try it out. I figured this would be a good way of showing how to implement a new concept or idea in Lasagne. **This use case of trying out new ideas and implementing new types of layers is extremely important to us, and we are trying to make it as easy as possible to use Lasagne in this way.**\n",
    "\n",
    "## Approach\n",
    "I decided to implement this idea in two steps. First, I added a `MultiplicativeGatingLayer`, which performs the following operation: `y(t, x1, x2) = t * x1 + (1 - t) * x2`. In other words, the first input `t` multiplicatively gates between the others `x1` and `x2`.\n",
    "\n",
    "This then makes it possible to use any layer we like for computing `t` and `x1` (and `x2` is taken to be the output of the previous layer). I implemented two \"macro functions\" on top of this: `highway_dense` and `highway_conv2d`. They create fully connected and 2D convolutional highway layers respectively.\n",
    "\n",
    "This two-step approach allows for some code reuse and easy implementation of different types of highway layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 540M\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, create a custom layer class for the multiplicative gating operation.** This is a layer with multiple input layers, three to be precise: `x`, `H(x)` and `T(x)`. In Lasagne, this means it needs to inherit from `MergeLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiplicativeGatingLayer(nn.layers.MergeLayer):\n",
    "    \"\"\"\n",
    "    Generic layer that combines its 3 inputs t, h1, h2 as follows:\n",
    "    y = t * h1 + (1 - t) * h2\n",
    "    \"\"\"\n",
    "    def __init__(self, gate, input1, input2, **kwargs):\n",
    "        incomings = [gate, input1, input2]\n",
    "        super(MultiplicativeGatingLayer, self).__init__(incomings, **kwargs)\n",
    "        assert gate.output_shape == input1.output_shape == input2.output_shape\n",
    "    \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return input_shapes[0]\n",
    "    \n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        return inputs[0] * inputs[1] + (1 - inputs[0]) * inputs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can define a macro function to create a dense highway layer.** Note that it does not take a `num_units` input argument: the number of outputs should always be the same as the number of inputs, so it is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highway_dense(incoming, Wh=nn.init.Orthogonal(), bh=nn.init.Constant(0.0),\n",
    "                  Wt=nn.init.Orthogonal(), bt=nn.init.Constant(-4.0),\n",
    "                  nonlinearity=nn.nonlinearities.rectify, **kwargs):\n",
    "    num_inputs = int(np.prod(incoming.output_shape[1:]))\n",
    "    # regular layer\n",
    "    l_h = nn.layers.DenseLayer(incoming, num_units=num_inputs, W=Wh, b=bh,\n",
    "                               nonlinearity=nonlinearity)\n",
    "    # gate layer\n",
    "    l_t = nn.layers.DenseLayer(incoming, num_units=num_inputs, W=Wt, b=bt,\n",
    "                               nonlinearity=T.nnet.sigmoid)\n",
    "    \n",
    "    return MultiplicativeGatingLayer(gate=l_t, input1=l_h, input2=incoming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can easily do the same for a 2D convolution highway layer.** As mentioned in the paper, we need to use 'same' convolutions here to ensure that the shape of `H(x)` and `T(x)` matches that of `x`.\n",
    "\n",
    "Unfortunately the implementation of 'same' convolutions in Theano using the default convolution operations `T.nnet.conv.conv2d` is a bit challenging. The default approach in Lasagne is to perform a 'full' convolution and then crop it, which can be slow. This is implemented in `lasagne.layers.Conv2DLayer`.\n",
    "\n",
    "To get an actual 'same' convolution, you could use one of the alternative convolution layer implementations that Lasagne provides, such as `lasagne.layers.dnn.Conv2DDNNLayer`, `lasagne.layers.corrmm.Conv2DMMLayer` or `lasagne.layers.cuda_convnet.Conv2DCCLayer`, all of which support the 'same' convolution mode properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highway_conv2d(incoming, filter_size,\n",
    "                   Wh=nn.init.Orthogonal(), bh=nn.init.Constant(0.0),\n",
    "                   Wt=nn.init.Orthogonal(), bt=nn.init.Constant(-4.0),\n",
    "                   nonlinearity=nn.nonlinearities.rectify, **kwargs):\n",
    "    num_channels = incoming.output_shape[1]\n",
    "    # regular layer\n",
    "    l_h = nn.layers.Conv2DLayer(incoming, num_filters=num_channels,\n",
    "                                filter_size=filter_size,\n",
    "                                border_mode='same', W=Wh, b=bh,\n",
    "                                nonlinearity=nonlinearity)\n",
    "    # gate layer\n",
    "    l_t = nn.layers.Conv2DLayer(incoming, num_filters=num_channels,\n",
    "                                filter_size=filter_size,\n",
    "                                border_mode='same', W=wt, b=bt,\n",
    "                                nonlinearity=T.nnet.sigmoid)\n",
    "    \n",
    "    return MultiplicativeGatingLayer(gate=l_t, input1=l_h, input2=incoming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import some helper code from the MNIST example bundled with Lasagne, and use it to build and train a highway model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist import load_data, train, create_iter_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim, batch_size=100,\n",
    "                num_hidden_units=20, num_hidden_layers=50):\n",
    "    \"\"\"Create a symbolic representation of a neural network with `intput_dim`\n",
    "    input nodes, `output_dim` output nodes, `num_hidden_layers` hidden layers\n",
    "    and `num_hidden_units` per hidden layer.\n",
    "    \n",
    "    The training function of this model must have a mini-batch size of\n",
    "    `batch_size`.\n",
    "    \"\"\"\n",
    "    l_in = nn.layers.InputLayer((batch_size, input_dim))\n",
    "    \n",
    "    # first, project it down to the desired number of units per layer\n",
    "    l_hidden1 = nn.layers.DenseLayer(l_in, num_units=num_hidden_units)\n",
    "    \n",
    "    # then stack highway layers on top of this\n",
    "    l_current = l_hidden1\n",
    "    for k in range(num_hidden_layers - 1):\n",
    "        l_current = highway_dense(l_current)\n",
    "        \n",
    "    # finally add an output layer\n",
    "    l_out = nn.layers.DenseLayer(\n",
    "        l_current, num_units=output_dim,\n",
    "        nonlinearity=nn.nonlinearities.softmax,\n",
    "    )\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we need to redefine the main function here. We cannot import it because we need it to use our version of\n",
    "`build_model`, not the one defined in mnist.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def main(num_epochs=50):\n",
    "    print(\"Loading data...\")\n",
    "    dataset = load_data()\n",
    "\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    output_layer = build_model(\n",
    "        input_dim=dataset['input_dim'],\n",
    "        output_dim=dataset['output_dim'],\n",
    "    )\n",
    "    iter_funcs = create_iter_functions(dataset, output_layer)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    now = time.time()\n",
    "    try:\n",
    "        for epoch in train(iter_funcs, dataset):\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch['number'], num_epochs, time.time() - now))\n",
    "            now = time.time()\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(epoch['train_loss']))\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(epoch['valid_loss']))\n",
    "            print(\"  validation accuracy:\\t\\t{:.2f} %%\".format(\n",
    "                epoch['valid_accuracy'] * 100))\n",
    "\n",
    "            if epoch['number'] >= num_epochs:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sander/python_modules/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n",
      "/home/sander/python_modules/lasagne/layers/helper.py:67: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch 1 of 50 took 5.111s\n",
      "  training loss:\t\t2.030891\n",
      "  validation loss:\t\t1.723327\n",
      "  validation accuracy:\t\t62.33 %%\n",
      "Epoch 2 of 50 took 5.135s\n",
      "  training loss:\t\t1.473506\n",
      "  validation loss:\t\t1.177809\n",
      "  validation accuracy:\t\t76.32 %%\n",
      "Epoch 3 of 50 took 5.040s\n",
      "  training loss:\t\t0.994532\n",
      "  validation loss:\t\t0.762730\n",
      "  validation accuracy:\t\t82.26 %%\n",
      "Epoch 4 of 50 took 5.199s\n",
      "  training loss:\t\t0.679902\n",
      "  validation loss:\t\t0.535956\n",
      "  validation accuracy:\t\t86.06 %%\n",
      "Epoch 5 of 50 took 5.134s\n",
      "  training loss:\t\t0.514816\n",
      "  validation loss:\t\t0.430797\n",
      "  validation accuracy:\t\t88.16 %%\n",
      "Epoch 6 of 50 took 5.120s\n",
      "  training loss:\t\t0.427798\n",
      "  validation loss:\t\t0.372764\n",
      "  validation accuracy:\t\t89.27 %%\n",
      "Epoch 7 of 50 took 5.271s\n",
      "  training loss:\t\t0.374679\n",
      "  validation loss:\t\t0.333487\n",
      "  validation accuracy:\t\t90.05 %%\n",
      "Epoch 8 of 50 took 5.325s\n",
      "  training loss:\t\t0.338382\n",
      "  validation loss:\t\t0.303995\n",
      "  validation accuracy:\t\t90.65 %%\n",
      "Epoch 9 of 50 took 5.467s\n",
      "  training loss:\t\t0.311969\n",
      "  validation loss:\t\t0.282334\n",
      "  validation accuracy:\t\t91.29 %%\n",
      "Epoch 10 of 50 took 5.595s\n",
      "  training loss:\t\t0.291058\n",
      "  validation loss:\t\t0.265502\n",
      "  validation accuracy:\t\t91.77 %%\n",
      "Epoch 11 of 50 took 5.205s\n",
      "  training loss:\t\t0.273657\n",
      "  validation loss:\t\t0.252176\n",
      "  validation accuracy:\t\t92.26 %%\n",
      "Epoch 12 of 50 took 5.125s\n",
      "  training loss:\t\t0.258541\n",
      "  validation loss:\t\t0.240773\n",
      "  validation accuracy:\t\t92.68 %%\n",
      "Epoch 13 of 50 took 5.232s\n",
      "  training loss:\t\t0.245016\n",
      "  validation loss:\t\t0.230503\n",
      "  validation accuracy:\t\t93.04 %%\n",
      "Epoch 14 of 50 took 5.148s\n",
      "  training loss:\t\t0.232775\n",
      "  validation loss:\t\t0.221477\n",
      "  validation accuracy:\t\t93.30 %%\n",
      "Epoch 15 of 50 took 5.211s\n",
      "  training loss:\t\t0.221683\n",
      "  validation loss:\t\t0.214196\n",
      "  validation accuracy:\t\t93.54 %%\n",
      "Epoch 16 of 50 took 5.249s\n",
      "  training loss:\t\t0.211427\n",
      "  validation loss:\t\t0.207892\n",
      "  validation accuracy:\t\t93.70 %%\n",
      "Epoch 17 of 50 took 5.194s\n",
      "  training loss:\t\t0.202148\n",
      "  validation loss:\t\t0.202096\n",
      "  validation accuracy:\t\t93.91 %%\n",
      "Epoch 18 of 50 took 5.252s\n",
      "  training loss:\t\t0.193941\n",
      "  validation loss:\t\t0.196544\n",
      "  validation accuracy:\t\t94.08 %%\n",
      "Epoch 19 of 50 took 5.280s\n",
      "  training loss:\t\t0.186415\n",
      "  validation loss:\t\t0.191673\n",
      "  validation accuracy:\t\t94.19 %%\n",
      "Epoch 20 of 50 took 5.237s\n",
      "  training loss:\t\t0.179737\n",
      "  validation loss:\t\t0.186329\n",
      "  validation accuracy:\t\t94.33 %%\n",
      "Epoch 21 of 50 took 5.187s\n",
      "  training loss:\t\t0.173368\n",
      "  validation loss:\t\t0.181993\n",
      "  validation accuracy:\t\t94.53 %%\n",
      "Epoch 22 of 50 took 5.063s\n",
      "  training loss:\t\t0.168538\n",
      "  validation loss:\t\t0.175252\n",
      "  validation accuracy:\t\t94.69 %%\n",
      "Epoch 23 of 50 took 5.202s\n",
      "  training loss:\t\t0.162272\n",
      "  validation loss:\t\t0.173084\n",
      "  validation accuracy:\t\t94.83 %%\n",
      "Epoch 24 of 50 took 5.097s\n",
      "  training loss:\t\t0.157075\n",
      "  validation loss:\t\t0.169722\n",
      "  validation accuracy:\t\t94.98 %%\n",
      "Epoch 25 of 50 took 5.062s\n",
      "  training loss:\t\t0.152455\n",
      "  validation loss:\t\t0.166322\n",
      "  validation accuracy:\t\t95.09 %%\n",
      "Epoch 26 of 50 took 5.051s\n",
      "  training loss:\t\t0.147829\n",
      "  validation loss:\t\t0.164570\n",
      "  validation accuracy:\t\t95.18 %%\n",
      "Epoch 27 of 50 took 5.065s\n",
      "  training loss:\t\t0.143443\n",
      "  validation loss:\t\t0.162692\n",
      "  validation accuracy:\t\t95.24 %%\n",
      "Epoch 28 of 50 took 5.077s\n",
      "  training loss:\t\t0.139459\n",
      "  validation loss:\t\t0.160020\n",
      "  validation accuracy:\t\t95.33 %%\n",
      "Epoch 29 of 50 took 5.268s\n",
      "  training loss:\t\t0.135873\n",
      "  validation loss:\t\t0.158794\n",
      "  validation accuracy:\t\t95.35 %%\n",
      "Epoch 30 of 50 took 5.091s\n",
      "  training loss:\t\t0.132091\n",
      "  validation loss:\t\t0.156907\n",
      "  validation accuracy:\t\t95.42 %%\n",
      "Epoch 31 of 50 took 5.023s\n",
      "  training loss:\t\t0.130013\n",
      "  validation loss:\t\t0.155049\n",
      "  validation accuracy:\t\t95.56 %%\n",
      "Epoch 32 of 50 took 5.026s\n",
      "  training loss:\t\t0.126594\n",
      "  validation loss:\t\t0.151907\n",
      "  validation accuracy:\t\t95.65 %%\n",
      "Epoch 33 of 50 took 5.107s\n",
      "  training loss:\t\t0.124951\n",
      "  validation loss:\t\t0.154000\n",
      "  validation accuracy:\t\t95.49 %%\n",
      "Epoch 34 of 50 took 5.147s\n",
      "  training loss:\t\t0.120938\n",
      "  validation loss:\t\t0.152238\n",
      "  validation accuracy:\t\t95.59 %%\n",
      "Epoch 35 of 50 took 5.147s\n",
      "  training loss:\t\t0.117289\n",
      "  validation loss:\t\t0.152455\n",
      "  validation accuracy:\t\t95.66 %%\n",
      "Epoch 36 of 50 took 5.042s\n",
      "  training loss:\t\t0.114447\n",
      "  validation loss:\t\t0.153111\n",
      "  validation accuracy:\t\t95.61 %%\n",
      "Epoch 37 of 50 took 5.043s\n",
      "  training loss:\t\t0.112000\n",
      "  validation loss:\t\t0.150884\n",
      "  validation accuracy:\t\t95.70 %%\n",
      "Epoch 38 of 50 took 5.039s\n",
      "  training loss:\t\t0.110244\n",
      "  validation loss:\t\t0.154455\n",
      "  validation accuracy:\t\t95.66 %%\n",
      "Epoch 39 of 50 took 5.057s\n",
      "  training loss:\t\t0.108189\n",
      "  validation loss:\t\t0.154380\n",
      "  validation accuracy:\t\t95.64 %%\n",
      "Epoch 40 of 50 took 5.134s\n",
      "  training loss:\t\t0.107190\n",
      "  validation loss:\t\t0.151688\n",
      "  validation accuracy:\t\t95.75 %%\n",
      "Epoch 41 of 50 took 5.271s\n",
      "  training loss:\t\t0.104705\n",
      "  validation loss:\t\t0.148006\n",
      "  validation accuracy:\t\t95.89 %%\n",
      "Epoch 42 of 50 took 5.082s\n",
      "  training loss:\t\t0.109415\n",
      "  validation loss:\t\t0.152791\n",
      "  validation accuracy:\t\t95.76 %%\n",
      "Epoch 43 of 50 took 5.185s\n",
      "  training loss:\t\t0.106737\n",
      "  validation loss:\t\t0.142674\n",
      "  validation accuracy:\t\t95.86 %%\n",
      "Epoch 44 of 50 took 5.265s\n",
      "  training loss:\t\t0.101955\n",
      "  validation loss:\t\t0.146293\n",
      "  validation accuracy:\t\t95.98 %%\n",
      "Epoch 45 of 50 took 5.250s\n",
      "  training loss:\t\t0.098156\n",
      "  validation loss:\t\t0.152230\n",
      "  validation accuracy:\t\t95.84 %%\n",
      "Epoch 46 of 50 took 5.111s\n",
      "  training loss:\t\t0.095668\n",
      "  validation loss:\t\t0.153339\n",
      "  validation accuracy:\t\t95.84 %%\n",
      "Epoch 47 of 50 took 5.027s\n",
      "  training loss:\t\t0.093911\n",
      "  validation loss:\t\t0.150652\n",
      "  validation accuracy:\t\t95.86 %%\n",
      "Epoch 48 of 50 took 5.031s\n",
      "  training loss:\t\t0.092623\n",
      "  validation loss:\t\t0.151234\n",
      "  validation accuracy:\t\t95.82 %%\n",
      "Epoch 49 of 50 took 5.030s\n",
      "  training loss:\t\t0.092943\n",
      "  validation loss:\t\t0.150491\n",
      "  validation accuracy:\t\t95.97 %%\n",
      "Epoch 50 of 50 took 5.012s\n",
      "  training loss:\t\t0.095758\n",
      "  validation loss:\t\t0.150384\n",
      "  validation accuracy:\t\t95.89 %%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lasagne.layers.dense.DenseLayer at 0x7f5741462e50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
